{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "sonnetsURL = \"http://www.gutenberg.org/files/1041/1041.txt\"\n",
    "sonnetString = urllib.request.urlopen(sonnetsURL).read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sonnetString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = sonnetString.find(\"  I\\r\\n\")\n",
    "end = sonnetString.find(\"End of Project Gutenberg\")\n",
    "filteredSonnetString = sonnetString[start:end].rstrip()\n",
    "print(filteredSonnetString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sonnetsList = re.split(\"  [A-Z]+\\r\\n\\r\\n\", filteredSonnetString)\n",
    "print(sonnetsList[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sonnetsList[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "sonnetsPath = 'sonnets'\n",
    "if not os.path.exists(sonnetsPath):\n",
    "    os.makedirs(sonnetsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(1).zfill(3))\n",
    "print(str(150).zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, sonnet in enumerate(sonnetsList):\n",
    "    if len(sonnet.strip()) > 0:\n",
    "        filename = str(index).zfill(3)+\".txt\"\n",
    "        pathname = os.path.join(sonnetsPath, filename)\n",
    "        f = open(pathname, \"w\")\n",
    "        f.write(sonnet.strip())\n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "sonnetsPath = 'sonnets'\n",
    "\n",
    "sonnetsCorpus = PlaintextCorpusReader(sonnetsPath, '.*txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sonnetsCorpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_summary(corpus):\n",
    "    print(\"This corpus has\")\n",
    "    print(\"  \", '{:,}'.format(len(sonnetsCorpus.fileids())), \"files\")\n",
    "    tokens = corpus.words()\n",
    "    print(\"  \", '{:,}'.format(len(tokens)), \"tokens\")\n",
    "    words = [word for word in tokens if word[0].isalpha()]\n",
    "    print(\"  \", '{:,}'.format(len(words)), \"words\")\n",
    "    print(\"  \", '{:,}'.format(len(set(words))), \"unique word types\")\n",
    "\n",
    "corpus_summary(sonnetsCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusTokens = sonnetsCorpus.words()\n",
    "print([word for word in corpusTokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_sentiments = {\n",
    "    \"pos\": {\n",
    "        \"love\": 1,\n",
    "        \"like\": .5\n",
    "    },\n",
    "    \"neg\": {\n",
    "        \"hate\": -1,\n",
    "        \"dislike\": -.5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "documents = {}\n",
    "for fileid in sonnetsCorpus.fileids():\n",
    "    text = sonnetsCorpus.raw(fileid).lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    score = 0\n",
    "    for polarity, words_dict in short_sentiments.items():\n",
    "        for word, value in words_dict.items():\n",
    "            score += tokens.count(word) * value\n",
    "    documents[fileid] = score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesFreqs = nltk.FreqDist(documents)\n",
    "mostFreqField = valuesFreqs.max()\n",
    "valuesFreqs.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sonnetsCorpus.raw(mostFreqField))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "for senti_synset in swn.senti_synsets('good'):\n",
    "    print(senti_synset, senti_synset.synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for senti_synset in swn.senti_synsets('wicked', 'a'):\n",
    "    print(senti_synset, senti_synset.synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"This is a good sentence.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treebank_to_wordnet_pos(treebank, skipWordNetPos=[]):\n",
    "    if \"NN\" in treebank and \"n\" not in skipWordNetPos:\n",
    "        return \"n\"\n",
    "    elif \"JJ\" in treebank and \"a\" not in skipWordNetPos:\n",
    "        return \"a\"\n",
    "    elif \"VB\" in treebank and \"v\" not in skipWordNetPos:\n",
    "        return \"v\"\n",
    "    elif \"RB\" in treebank and \"r\" not in skipWordNetPos:\n",
    "        return \"r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, treebank in tagged:\n",
    "    wordnet_pos = treebank_to_wordnet_pos(treebank, [\"v\"])\n",
    "    if wordnet_pos:\n",
    "        print(word, wordnet_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, treebank in tagged:\n",
    "    wordnet_pos = treebank_to_wordnet_pos(treebank, [\"v\"])\n",
    "    if wordnet_pos: # only print matches\n",
    "        print(word)\n",
    "        for senti_synset in swn.senti_synsets(word, wordnet_pos):\n",
    "            print(\"  \", senti_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_score_from_tagged(token, treebank, skipWordNetPos=[]):\n",
    "    wordnet_pos = treebank_to_wordnet_pos(treebank, skipWordNetPos)\n",
    "    if wordnet_pos: # only print matches\n",
    "        senti_synsets = list(swn.senti_synsets(token, wordnet_pos))\n",
    "        if senti_synsets:\n",
    "            return senti_synsets[0].pos_score() - senti_synsets[0].neg_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, treebank in tagged:\n",
    "    score = get_sentiment_score_from_tagged(word, treebank, [\"v\"])\n",
    "    if score:\n",
    "        print(word, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_data_from_tokens(tokens, skipWordNetPos=[]):\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    tokens_score = 0\n",
    "    for word, treebank in tagged:\n",
    "        score = get_sentiment_score_from_tagged(word, treebank, skipWordNetPos)\n",
    "        if score:\n",
    "            tokens_score += score\n",
    "            if score > 0:\n",
    "                positives.append(word.lower())\n",
    "            else:\n",
    "                negatives.append(word.lower())\n",
    "    return tokens_score, set(positives), set(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment_data_from_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiments_data_from_corpus(corpus, skipWordNetPos=[]):\n",
    "    documents = {}\n",
    "    all_positives = []\n",
    "    all_negatives = []\n",
    "    for fileid in corpus.fileids():\n",
    "        tokens = corpus.words(fileid)\n",
    "        score, positives, negatives = get_sentiment_data_from_tokens(tokens, skipWordNetPos)\n",
    "        documents[fileid] = score\n",
    "        [all_positives.append(positive) for positive in positives]\n",
    "        [all_negatives.append(negative) for negative in negatives]\n",
    "    return documents, set(all_positives), set(all_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sonnetsSentimentValues, sonnetsPositives, sonnetsNegatives = get_sentiments_data_from_corpus(sonnetsCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sonnetsSentimentFreqs = nltk.FreqDist(sonnetsSentimentValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnetsSentimentFreqs.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sonnetsSentimentFreqs.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.mean([val for doc, val in sonnetsSentimentFreqs.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_html_for_sentiment_data(text, positives, negatives):\n",
    "    # the regular expression combines all of the positive and negative words for a search, e.g. (love|like)\n",
    "    # it then surrounds the word found in parentheses with styling, green for positive, red for negative\n",
    "    if len(negatives) > 0:\n",
    "        text = re.sub(r'\\b(' + '|'.join(negatives) + r')\\b', r'<span style=\"color:red\">\\1</span>', text)  \n",
    "    if len(positives) > 0:\n",
    "        text = re.sub(r'\\b(' + '|'.join(positives) + r')\\b', r'<span style=\"color:green\">\\1</span>', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileid = sonnetsSentimentFreqs.max() # most positive\n",
    "text = sonnetsCorpus.raw(fileid)\n",
    "html = get_html_for_sentiment_data(text, sonnetsPositives, sonnetsNegatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h2>\" + fileid + \"</h2><pre>\" + html + \"</pre>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileid = sonnetsSentimentFreqs.most_common()[-1][0] # most negative (fileid of the last element in the most common list)\n",
    "text = sonnetsCorpus.raw(fileid)\n",
    "html = get_html_for_sentiment_data(text, sonnetsPositives, sonnetsNegatives)\n",
    "HTML(\"<h2>\" + fileid + \"</h2><pre>\" + html + \"</pre>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise (if time): In the cell below, write some code that will loop through all the sonnets and print them all in color-coded HTML, as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
