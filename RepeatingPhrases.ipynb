{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spring 2018, Week 1\n",
    "To use this notebook, you'll need NLTK's data packages installed. To install these packages, run the cell below. <b>Run the cell below only if you have not downloaded the NLTK data!</b> Running the cell will cause a dialog box to open. Select \"all\" and press \"Download.\" You can close the window once the download is complete.\n",
    "\n",
    "For today's workgroup, we'll talk about finding repeated phrases (or ngrams) in texts. We'll focus on a single text, the novel <i>Emma</i>, by Jane Austen. As we walk through the cells below, you can make notes for yourself, either by creating a new Markdown Cell (click Insert above, then use the dialog box to change the cell to a markdown cell) or by inserting comments in the code using the # symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell only if you don't have the NLTK data!!\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "emmaTokens = list(nltk.corpus.gutenberg.words(\"austen-emma.txt\")) #read in the tokens file from NLTK library\n",
    "emmaWords = [word for word in emmaTokens if word[0].isalpha()] #create a list of words from the tokens file\n",
    "print(emmaWords[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emmaText = nltk.Text(emmaWords) \n",
    "emmaText.collocations(20) #show the most frequent bigrams, excluding all stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emmaFirstSixWords = \" \".join(emmaWords[:6]) #join the first six words of Emma with spaces\n",
    "print(\"first six words of Emma: \", emmaFirstSixWords)\n",
    "emmaBigrams = list(nltk.ngrams(emmaWords, 2)) #create bigrams from emmaWords\n",
    "emmaBigrams[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(emmaWords))\n",
    "print(len(emmaBigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma4grams = list(nltk.ngrams(emmaWords, 4)) #create 4-grams of Emma\n",
    "emma4gramsFreqs = nltk.FreqDist(emma4grams) #determine frequency of 4-grams\n",
    "for words, count in emma4gramsFreqs.most_common(15):\n",
    "    print(count, \" \".join(list(words))) #show the count and then create a string from the 4-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "emmaText.dispersion_plot([\"I do not know\"]) #Won't work because phrases are not in the emmaText tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma4gramsText = nltk.Text(emma4grams)\n",
    "emma4gramsText.dispersion_plot([(\"I\",\"do\",\"not\",\"know\")]) #convert the 4grams into a text and then plot this tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emma4gramsTokens = [\" \".join(gram) for gram in emma4grams] #joins the 4grams into a list of tuples\n",
    "nltk.Text(emma4gramsTokens).dispersion_plot([(\"I do not know\")]) #converts list of tuples to a string and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngramsFreqs = [] #keep track of the last set of repeating phrases\n",
    "for length in range(2, len(emmaWords)):  #create a range from 2 to the length of the whole text\n",
    "    ngrams = list(nltk.ngrams(emmaWords, length))  #get ngrams for a specified length\n",
    "    freqs = nltk.FreqDist(ngrams) #get the frequencies for those ngrams\n",
    "    freqs = [(ngram, count) for ngram, count in freqs.items() if count > 1] #filter out frequencies that don't repeat\n",
    "    if len(freqs) > 0:  #if we have at least one repeating phrase\n",
    "        ngramsFreqs = freqs  #new set of frequenices\n",
    "    else:\n",
    "        break  #if we've filtered out all the frequencies, then break out of the loop\n",
    "\n",
    "for ngram, count in ngramsFreqs:\n",
    "    print(\"ngram of\", len(ngram), \"words occurring\", count, \"times:\", \" \".join(list(ngram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = list(range(0,10))\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "numberBins = np.array_split(numbers, 5) #divide our numbers into five equal bins\n",
    "print(\"number of bins:\", len(numberBins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numberBins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma4gramsSegments = np.array_split(emma4gramsTokens, 10)\n",
    "[len(segment) for segment in emma4gramsSegments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idkCounts = [list(segment).count((\"I do not know\")) for segment in emma4gramsSegments]\n",
    "idkCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "line = plt.plot(idkCounts, label=\"I do not know\")\n",
    "plt.ylim(0) #y axis at zero\n",
    "plt.legend(handles=line) #add legend\n",
    "plt.show() #flush out the line chart\n",
    "emma4gramsText.dispersion_plot([(\"I\",\"do\",\"not\",\"know\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xaxis = range(0, len(idkCounts))\n",
    "bar = plt.bar(xaxis, idkCounts, label=\"I do not know\")\n",
    "plt.legend(handles=[bar])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searches = [\"I do not know\", \"I have no doubt\"] #line plot comparison of \"I do not know\" and \"I have no doubt\"\n",
    "lines = []\n",
    "for search in searches:\n",
    "    line, = plt.plot([list(segment).count(search) for segment in emma4gramsSegments], label=search)\n",
    "    lines.append(line)\n",
    "plt.legend(handles=lines)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list1 = [1,2,3]\n",
    "list2 = [1,5,7]\n",
    "plt.plot(range(3), list1, range(3), list2)\n",
    "plt.show()\n",
    "np.corrcoef(list1, list2)[0,1] #returns a matrix of values, but we just want the top right value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list2.reverse()\n",
    "plt.plot(range(3), list1, range(3), list2)\n",
    "plt.show()\n",
    "np.corrcoef(list1, list2)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idkCounts = [list(segment).count(\"I do not know\") for segment in emma4gramsSegments]\n",
    "iHaveNoDoubtCounts = [list(segment).count(\"I have no doubt\") for segment in emma4gramsSegments]\n",
    "print(idkCounts)\n",
    "print(iHaveNoDoubtCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.corrcoef(idkCounts, iHaveNoDoubtCounts)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma4gramsMostFrequent = [\" \".join(words) for words, count in emma4gramsFreqs.most_common(15)] \n",
    "        # create a list of the top 15 most frequent 4grams\n",
    "print(emma4gramsMostFrequent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we first go through the most frequent 4grams in Emma, and for every segment in the Emma 4grams segments, we create a list of the counts of the 4gram and add that count to the segments counts dictionary.\n",
    "\n",
    "Then, we create another dictionary item of correlations. We go through each of the ngram and count tuples in the emma segments couunts, and for every one of those counts, we do a numpy correlation coefficient against \"I do not know\".\n",
    "\n",
    "Then, we create a FreqDist of the correlations dictionary so the items are ordered by frequency, and we plot that on a line graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma4gramsSegmentsCounts = {}  #build a dictionary of counts for each search item\n",
    "for search in emma4gramsMostFrequent:\n",
    "    emma4gramsSegmentsCounts[search] = [list(segment).count(search) for segment in emma4gramsSegments]\n",
    "#print(emma4gramsSegmentsCounts)\n",
    "\n",
    "iDoNotKnowCorrelations = {}  #build a dictionary of correlation values for \"I do not know\"\n",
    "for ngram, counts in emma4gramsSegmentsCounts.items():\n",
    "    iDoNotKnowCorrelations[ngram] = np.corrcoef(emma4gramsSegmentsCounts[\"I do not know\"], counts)[0,1]\n",
    "#print(iDoNotKnowCorrelations)\n",
    "\n",
    "iDoNotKnowCorrelationsFreqs = nltk.FreqDist(iDoNotKnowCorrelations)\n",
    "print(iDoNotKnowCorrelationsFreqs.most_common())\n",
    "iDoNotKnowCorrelationsFreqs.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
